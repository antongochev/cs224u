{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colors import ColorsCorpusReader\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_color_describer import (\n",
    "    ContextualColorDescriber, create_example_dataset)\n",
    "import utils\n",
    "from utils import START_SYMBOL, END_SYMBOL, UNK_SYMBOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS_SRC_FILENAME = os.path.join(\n",
    "    \"data\", \"colors\", \"filteredCorpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_corpus = ColorsCorpusReader(\n",
    "    COLORS_SRC_FILENAME,\n",
    "    word_count=2,\n",
    "    normalize_colors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_examples = list(dev_corpus.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13890"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rawcols, dev_texts = zip(*[[ex.colors, ex.contents] for ex in dev_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rawcols_train, dev_rawcols_test, dev_texts_train, dev_texts_test = \\\n",
    "    train_test_split(dev_rawcols, dev_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_words(corpus):\n",
    "    all_words = []\n",
    "    # remove the punctuation before splitting\n",
    "    for s in corpus:\n",
    "        for string in strip_punctuation(s.lower()).split():\n",
    "            # remove suffixes from each word\n",
    "            all_words.append(strip_suffix(string, ['er', 'est', 'ish']))\n",
    "    \n",
    "    return [w for w in all_words if all_words.count(w) <= 1]\n",
    "    \n",
    "def tokenize_example(s):\n",
    "\n",
    "    s_strip = []\n",
    "    # remove the punctuation before splitting\n",
    "    for string in strip_punctuation(s.lower()).split():\n",
    "        # remove suffixes from each word\n",
    "        s_strip.append(strip_suffix(string, ['er', 'est', 'ish']))\n",
    "    \n",
    "    s_final = s_strip\n",
    "\n",
    "    return [START_SYMBOL] + s_final + [END_SYMBOL]\n",
    "\n",
    "def strip_suffix(s, suffixes):\n",
    "    result = s\n",
    "    for suffix in suffixes:\n",
    "        result = s[:(len(s) - len(suffix))] if s.endswith(suffix) else s\n",
    "        if result != s:\n",
    "            break\n",
    "            \n",
    "    return result       \n",
    "\n",
    "def strip_punctuation(s):\n",
    "    punc = s.maketrans(dict.fromkeys(string.punctuation))\n",
    "    return s.translate(punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_seqs_train = [tokenize_example(s) for s in dev_texts_train]\n",
    "\n",
    "dev_seqs_test = [tokenize_example(s) for s in dev_texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vocab = sorted({w for toks in dev_seqs_train for w in toks})\n",
    "\n",
    "dev_vocab += [UNK_SYMBOL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '10', '2', '2nd', '6', '</s>', '<s>', 'a', 'again', 'ahaha']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "987"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Colours represnetation improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsl_hsv(colour):\n",
    "    \"\"\"\n",
    "    Converts a color from hsl format to hsv \n",
    "    https://en.wikipedia.org/wiki/HSL_and_HSV#HSV_to_HSL\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    colour: a list of float\n",
    "        Represent the three colours in HSL format.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    typ: list of float\n",
    "        The transformed to HSV format clour values.\n",
    "     \n",
    "    \"\"\"\n",
    "    H, S, L = colour\n",
    "    H_v = H\n",
    "    V = L + S * min(L, 1 - L)\n",
    "    S_v = 0 if V == 0 else 2 * (1 - L/V)\n",
    "\n",
    "    return [H_v, V, S_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19444444444444445, 0.165, 0.6666666666666667]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsv_t = hsl_hsv(dev_rawcols_train[0][0])\n",
    "hsv_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmath as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice source code https://github.com/futurulus/coop-nets/blob/01b1710b71358b224494d3329cc31b3cff9e10f6/vectorizers.py#L599\n",
    "def represent_color_context(colors):\n",
    "\n",
    "    result = []\n",
    "    for hslcolour in colors:\n",
    "        f_jkl = []\n",
    "        for j, k, l in product((0, 1, 2), repeat=3):    \n",
    "            h, s, v = hsl_hsv(hslcolour)\n",
    "            f_jkl.append(\n",
    "                cm.exp(-2j * cm.pi * (j*h/360 + k*s/200 + l*v/200))\n",
    "            )\n",
    "\n",
    "        real = []\n",
    "        imag = []\n",
    "        for f in f_jkl:\n",
    "            real.append(f.real)\n",
    "            imag.append(f.imag)\n",
    "        result.append(np.transpose(real + imag))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.        ,  0.99978068,  0.99912283,  0.99982235,  0.99920834,\n",
       "         0.99815605,  0.99928947,  0.99828099,  0.99683462,  0.99998654,\n",
       "         0.99965859,  0.99889216,  0.99971112,  0.99898852,  0.99782774,\n",
       "         0.99908051,  0.99796352,  0.99640879,  0.99994618,  0.9995096 ,\n",
       "         0.9986346 ,  0.99957299,  0.99874182,  0.99747257,  0.99884466,\n",
       "         0.9976192 ,  0.99595615,  0.        , -0.02094242, -0.04187565,\n",
       "        -0.01884844, -0.03978301, -0.06070012, -0.03769018, -0.05860946,\n",
       "        -0.07950302, -0.00518748, -0.02612848, -0.04705802, -0.02403475,\n",
       "        -0.04496585, -0.06587722, -0.04287347, -0.06378723, -0.08467301,\n",
       "        -0.01037483, -0.03131384, -0.05223913, -0.02922041, -0.05014748,\n",
       "        -0.07105255, -0.04805561, -0.06896329, -0.08984073]),\n",
       " array([ 1.        ,  0.99978068,  0.99912283,  0.99973342,  0.99903063,\n",
       "         0.99788963,  0.99893383,  0.99774794,  0.9961244 ,  0.99999921,\n",
       "         0.99975349,  0.99906925,  0.99970352,  0.99897435,  0.99780699,\n",
       "         0.99887484,  0.9976626 ,  0.99601274,  0.99999682,  0.99972471,\n",
       "         0.99901409,  0.99967204,  0.99891648,  0.99772276,  0.99881427,\n",
       "         0.99757567,  0.9958995 ,  0.        , -0.02094242, -0.04187565,\n",
       "        -0.02308865, -0.04402043, -0.06493289, -0.046165  , -0.06707497,\n",
       "        -0.08795551, -0.00126052, -0.02220264, -0.04313503, -0.02434882,\n",
       "        -0.04527969, -0.0661907 , -0.04742413, -0.06833259, -0.08921107,\n",
       "        -0.00252103, -0.02346283, -0.04439434, -0.02560894, -0.04653887,\n",
       "        -0.06744839, -0.04868319, -0.0695901 , -0.09046649]),\n",
       " array([ 1.        ,  0.99999046,  0.99996184,  0.99956862,  0.9994308 ,\n",
       "         0.99927391,  0.99827485,  0.99800886,  0.99772383,  0.99998604,\n",
       "         0.99995342,  0.99990172,  0.99939946,  0.99923857,  0.99905861,\n",
       "         0.99795064,  0.99766162,  0.99735356,  0.99994415,  0.99988845,\n",
       "         0.99981367,  0.99920239,  0.99901844,  0.99881542,  0.99759856,\n",
       "         0.99728651,  0.99695543,  0.        , -0.00436798, -0.00873587,\n",
       "        -0.02936967, -0.03373548, -0.03810065, -0.058714  , -0.06307388,\n",
       "        -0.06743255, -0.00528444, -0.00965231, -0.01401999, -0.03465142,\n",
       "        -0.03901644, -0.04338072, -0.0639885 , -0.06834692, -0.07270403,\n",
       "        -0.01056874, -0.01493637, -0.01930372, -0.03993221, -0.04429632,\n",
       "        -0.04865959, -0.06926123, -0.07361805, -0.07797347])]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "represent_color_context(dev_rawcols_train[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cols_train = [represent_color_context(colors) for colors in dev_rawcols_train]\n",
    "\n",
    "dev_cols_test = [represent_color_context(colors) for colors in dev_rawcols_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_HOME = os.path.join('data', 'glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_embedding(vocab, glove_base_filename='glove.6B.50d.txt'):\n",
    "    pass\n",
    "\n",
    "    glove = utils.glove2dict(\n",
    "        os.path.join(GLOVE_HOME, glove_base_filename)\n",
    "    )\n",
    "\n",
    "    embedding, vocab_ex = \\\n",
    "        utils.create_pretrained_embedding(glove, vocab)\n",
    "\n",
    "    return embedding, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_glove_embedding, dev_glove_vocab = create_glove_embedding(dev_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Develop the colour context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_color_describer import Decoder, Encoder\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorContextLSTMEncoder(Encoder):\n",
    "    def __init__(self, color_dim, *args, **kwargs):\n",
    "        super().__init__(color_dim, *args, **kwargs)\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.color_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorContextLSTMDecoder(Decoder):\n",
    "    def __init__(self, color_dim, *args, **kwargs):\n",
    "        self.color_dim = color_dim\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            batch_first=True)\n",
    "\n",
    "\n",
    "    def get_embeddings_ext(self, word_seqs, target_colors=None):\n",
    "        \"\"\"\n",
    "        You can assume that `target_colors` is a tensor of shape\n",
    "        (m, n), where m is the length of the batch (same as\n",
    "        `word_seqs.shape[0]`) and n is the dimensionality of the\n",
    "        color representations the model is using. The goal is\n",
    "        to attach each color vector i to each of the tokens in\n",
    "        the ith sequence of (the embedded version of) `word_seqs`.\n",
    "\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE\n",
    "        embedding_word_seqs = self.embedding(word_seqs)\n",
    "        elen = embedding_word_seqs.shape[1]\n",
    "        tc_extended = target_colors\n",
    "        if len(target_colors.shape) < len(embedding_word_seqs.shape):\n",
    "            tc_extended = torch.unsqueeze(target_colors, 1)\n",
    "        repeat_vec = torch.LongTensor([elen/tc_extended.shape[1]])\n",
    "        repeated_tc = torch.repeat_interleave(tc_extended, repeat_vec, dim=1)\n",
    "\n",
    "        return torch.cat((embedding_word_seqs, repeated_tc), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizedInputLSTMDescriber(ContextualColorDescriber):\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        encoder = ColorContextLSTMEncoder(\n",
    "            color_dim=self.color_dim,\n",
    "            hidden_dim=self.hidden_dim)\n",
    "\n",
    "        decoder = ColorContextLSTMDecoder(\n",
    "            color_dim=self.color_dim,\n",
    "            vocab_size=self.vocab_size,\n",
    "            embedding=self.embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_dim=self.hidden_dim)\n",
    "\n",
    "        return ColorizedEncoderDecoder(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorContextEncoder(Encoder):\n",
    "    def __init__(self, color_dim, num_layers=2, *args, **kwargs):\n",
    "        super().__init__(color_dim, *args, **kwargs)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.color_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorContextDecoder(Decoder):\n",
    "    def __init__(self, color_dim, num_layers=2, *args, **kwargs):\n",
    "        self.color_dim = color_dim\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.color_dim + self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,            \n",
    "            batch_first=True)\n",
    "\n",
    "\n",
    "    def get_embeddings(self, word_seqs, target_colors=None):\n",
    "        \"\"\"\n",
    "        You can assume that `target_colors` is a tensor of shape\n",
    "        (m, n), where m is the length of the batch (same as\n",
    "        `word_seqs.shape[0]`) and n is the dimensionality of the\n",
    "        color representations the model is using. The goal is\n",
    "        to attach each color vector i to each of the tokens in\n",
    "        the ith sequence of (the embedded version of) `word_seqs`.\n",
    "\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE HERE\n",
    "        embedding_word_seqs = self.embedding(word_seqs)\n",
    "        elen = embedding_word_seqs.shape[1]\n",
    "        tc_extended = target_colors\n",
    "        if len(target_colors.shape) < len(embedding_word_seqs.shape):\n",
    "            tc_extended = torch.unsqueeze(target_colors, 1)\n",
    "        repeat_vec = torch.LongTensor([elen/tc_extended.shape[1]])\n",
    "        repeated_tc = torch.repeat_interleave(tc_extended, repeat_vec, dim=1)\n",
    "        \n",
    "        return torch.cat((embedding_word_seqs, repeated_tc), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def test_get_embeddings(decoder_class):\n",
    "#     \"\"\"\n",
    "#     It's assumed that the input to this will be `ColorContextDecoder`.\n",
    "#     You pass in the class, and the function initalizes it with the test\n",
    "#     parameters.\n",
    "#     \"\"\"\n",
    "# dec = decoder_class(\n",
    "#     color_dim=3,   # For these, we mainly want *different*\n",
    "#     vocab_size=10, # dimensions so that we reliably get\n",
    "#     embed_dim=4,   # dimensionality errors if something\n",
    "#     hidden_dim=5)  # isn't working.\n",
    "\n",
    "# This step just changes the embedding to one with values\n",
    "# that are easy to inspect and definitely will not change\n",
    "# between runs:\n",
    "embedding = nn.Embedding.from_pretrained(\n",
    "    torch.FloatTensor([\n",
    "        [10, 11, 12, 13],\n",
    "        [14, 15, 16, 17],\n",
    "        [18, 19, 20, 21]]))\n",
    "\n",
    "# These are the incoming sequences -- lists of indices\n",
    "# into the rows of `dec.embedding`:\n",
    "word_seqs = torch.tensor([\n",
    "    [0,1,2],\n",
    "    [2,0,1]])\n",
    "\n",
    "# Target colors as small floats that will be easy to track:\n",
    "target_colors = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [0.7, 0.8, 0.9]])\n",
    "\n",
    "# The desired return value: one list of tensors for each of\n",
    "# the two sequences in `word_seqs`. Each index is replaced\n",
    "# with its vector from `dec.embedding` and has the\n",
    "# corrresponding color from `target_colors` appended to it.\n",
    "expected = torch.tensor([\n",
    "    [[10., 11., 12., 13.,  0.1,  0.2,  0.3],\n",
    "     [14., 15., 16., 17.,  0.1,  0.2,  0.3],\n",
    "     [18., 19., 20., 21.,  0.1,  0.2,  0.3]],\n",
    "\n",
    "    [[18., 19., 20., 21.,  0.7,  0.8,  0.9],\n",
    "     [10., 11., 12., 13.,  0.7,  0.8,  0.9],\n",
    "     [14., 15., 16., 17.,  0.7,  0.8,  0.9]]])\n",
    "\n",
    "# result = dec.get_embeddings(word_seqs, target_colors=target_colors)\n",
    "\n",
    "# assert expected.shape == result.shape, \\\n",
    "#     \"Expected shape {}; got shape {}\".format(expected.shape, result.shape)\n",
    "\n",
    "# assert torch.all(expected.eq(result)), \\\n",
    "#     (\"Your result has the desired shape but the values aren't correct. \"\n",
    "#      \"Here's what your function creates; compare it with `expected` \"\n",
    "#      \"from the test:\\n{}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_word_seqs = embedding(word_seqs)\n",
    "elen = embedding_word_seqs.shape[1]\n",
    "tc_extended = target_colors\n",
    "if len(target_colors.shape) < len(embedding_word_seqs.shape):\n",
    "    tc_extended = torch.unsqueeze(target_colors, 1)\n",
    "repeat_vec = torch.LongTensor([elen/tc_extended.shape[1]])\n",
    "repeated_tc = torch.repeat_interleave(tc_extended, repeat_vec, dim=1)\n",
    "result = torch.cat((embedding_word_seqs, repeated_tc), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10.0000, 11.0000, 12.0000, 13.0000,  0.1000,  0.2000,  0.3000],\n",
       "         [14.0000, 15.0000, 16.0000, 17.0000,  0.1000,  0.2000,  0.3000],\n",
       "         [18.0000, 19.0000, 20.0000, 21.0000,  0.1000,  0.2000,  0.3000]],\n",
       "\n",
       "        [[18.0000, 19.0000, 20.0000, 21.0000,  0.7000,  0.8000,  0.9000],\n",
       "         [10.0000, 11.0000, 12.0000, 13.0000,  0.7000,  0.8000,  0.9000],\n",
       "         [14.0000, 15.0000, 16.0000, 17.0000,  0.7000,  0.8000,  0.9000]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 7])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_color_describer import EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizedEncoderDecoder(EncoderDecoder):\n",
    "\n",
    "    def forward(self,\n",
    "            color_seqs,\n",
    "            word_seqs,\n",
    "            seq_lengths=None,\n",
    "            hidden=None,\n",
    "            targets=None):\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = self.encoder(color_seqs)\n",
    "\n",
    "        # Extract the target colors from `color_seqs` and\n",
    "        # feed them to the decoder, which already has a\n",
    "        # `target_colors` keyword.\n",
    "\n",
    "        result = []\n",
    "        for cs in color_seqs:\n",
    "            result.append(np.array(cs[2]))\n",
    "        t_colours = torch.tensor(result)\n",
    "\n",
    "        output, hidden =  self.decoder(\n",
    "            target_colors=t_colours,\n",
    "            word_seqs=word_seqs,\n",
    "            seq_lengths=seq_lengths,\n",
    "            hidden=hidden)\n",
    "\n",
    "        if self.training:\n",
    "            return output\n",
    "        else:\n",
    "            return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_color_describer import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizedInputDescriber(ContextualColorDescriber):\n",
    "    def __init__(self, *args, num_layers=2, **kwargs):\n",
    "        self.num_layers = num_layers\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def build_graph(self):\n",
    "\n",
    "        encoder = ColorContextEncoder(\n",
    "            color_dim=self.color_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers)\n",
    "\n",
    "        decoder = ColorContextDecoder(\n",
    "            color_dim=self.color_dim,\n",
    "            vocab_size=self.vocab_size,\n",
    "            embedding=self.embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_dim=self.hidden_dim, \n",
    "            num_layers=self.num_layers)\n",
    "\n",
    "        return ColorizedEncoderDecoder(\n",
    "            encoder=encoder,\n",
    "            decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_full_system(describer_class):\n",
    "    toy_color_seqs, toy_word_seqs, toy_vocab = create_example_dataset(\n",
    "        group_size=50, vec_dim=2)\n",
    "\n",
    "    toy_color_seqs_train, toy_color_seqs_test, toy_word_seqs_train, toy_word_seqs_test = \\\n",
    "        train_test_split(toy_color_seqs, toy_word_seqs)\n",
    "\n",
    "    toy_mod = describer_class(toy_vocab)\n",
    "\n",
    "    _ = toy_mod.fit(toy_color_seqs_train, toy_word_seqs_train)\n",
    "\n",
    "    return toy_mod.listener_accuracy(toy_color_seqs_test, toy_word_seqs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 1000 of 1000; error is 0.11071182787418365"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_full_system(ColorizedInputDescriber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ColorizedInputDescriber(\n",
    "    dev_glove_vocab,\n",
    "    embedding=dev_glove_embedding,\n",
    "    early_stopping=True,\n",
    "    num_layers=3,\n",
    "    eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 19. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 39.94683003425598"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ColorizedInputDescriber(\n",
       "\tbatch_size=1028,\n",
       "\tmax_iter=1000,\n",
       "\teta=0.001,\n",
       "\toptimizer_class=<class 'torch.optim.adam.Adam'>,\n",
       "\tl2_strength=0,\n",
       "\tgradient_accumulation_steps=1,\n",
       "\tmax_grad_norm=None,\n",
       "\tvalidation_fraction=0.1,\n",
       "\tearly_stopping=True,\n",
       "\tn_iter_no_change=10,\n",
       "\twarm_start=False,\n",
       "\ttol=1e-05,\n",
       "\thidden_dim=50,\n",
       "\tembed_dim=50,\n",
       "\tembedding=[[ 0.1394268  -0.47498924 -0.22497068 ... -0.2220264   0.13568444\n",
       "  -0.13516782]\n",
       " [-0.14751     0.55556     1.0764     ... -0.3635      0.12941\n",
       "   0.18798   ]\n",
       " [-0.11098     0.86724     0.78114    ... -0.61752     0.59103\n",
       "   0.28649   ]\n",
       " ...\n",
       " [-0.23299    -0.5428     -0.4657     ...  0.070107    0.083831\n",
       "   0.46851   ]\n",
       " [-1.5754      0.45398    -0.37413    ...  0.56113     0.24468\n",
       "   0.43962   ]\n",
       " [ 0.23474262 -0.14531334  0.45088101 ... -0.42164843 -0.45073382\n",
       "  -0.03124779]],\n",
       "\tfreeze_embedding=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dev_cols_train, dev_seqs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_evaluate(trained_model, color_seqs_test, texts_test):\n",
    "    \n",
    "    # `word_seqs_test` is a list of strings, so tokenize each of\n",
    "    # its elements:\n",
    "    tok_seqs = [tokenize_example(s) for s in texts_test]\n",
    "\n",
    "    col_seqs = [represent_color_context(colors)\n",
    "                for colors in color_seqs_test]\n",
    "\n",
    "    return trained_model.evaluate(col_seqs, tok_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'listener_accuracy': 0.38180247624532104, 'corpus_bleu': 0.5094102047245922}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_and_evaluate(model, dev_rawcols_test, dev_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
