{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import rel_ext\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import utils\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ext_data_home = os.path.join('data', 'rel_ext_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = rel_ext.Corpus(os.path.join(rel_ext_data_home, 'corpus.tsv.gz'))\n",
    "kb = rel_ext.KB(os.path.join(rel_ext_data_home, 'kb.tsv.gz'))\n",
    "dataset = rel_ext.Dataset(corpus, kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tiny': Corpus with 14,966 examples; KB with 2,201 triples,\n",
       " 'train': Corpus with 251,793 examples; KB with 34,435 triples,\n",
       " 'dev': Corpus with 64,937 examples; KB with 9,248 triples,\n",
       " 'all': Corpus with 331,696 examples; KB with 45,884 triples}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = dataset.build_splits(\n",
    "    split_names=['tiny', 'train', 'dev'],\n",
    "    split_fracs=[0.05, 0.75, 0.20],\n",
    "    seed=1)\n",
    "\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_bag_of_non_stop_words_featurizer(kbt, corpus, feature_counter):\n",
    "    subject_object_suffix = \"_SO\"\n",
    "    object_subject_suffix = \"_OS\"\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = remove_stop_words(ex.middle)\n",
    "        for word in words:\n",
    "            feature_counter[word + subject_object_suffix] += 1 \n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = remove_stop_words(ex.middle)\n",
    "        for word in words:\n",
    "            feature_counter[word + object_subject_suffix] += 1\n",
    "\n",
    "    return feature_counter\n",
    "\n",
    "def directional_bag_of_words_right_featurizer(kbt, corpus, feature_counter):\n",
    "    subject_object_suffix = \"_SO\"\n",
    "    object_subject_suffix = \"_OS\"\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = remove_stop_words(ex.right)\n",
    "        for word in words:\n",
    "            feature_counter[word + subject_object_suffix] += 1  \n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = remove_stop_words(ex.right)\n",
    "        for word in words:\n",
    "            feature_counter[word + subject_object_suffix] += 1 \n",
    "\n",
    "    return feature_counter\n",
    "\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    words = []\n",
    "    for word in sentence.split(' '):\n",
    "        if word.lower() not in spacy_stopwords:\n",
    "            words.append(word)\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_mentions_featurizer(kbt, corpus, feature_counter):\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.left.split(' ') + ex.right.split(' ')\n",
    "        for word in words:\n",
    "            if word == kbt.sbj or word == kbt.obj:\n",
    "                feature_counter[word] += 1          \n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.left.split(' ') + ex.right.split(' ')\n",
    "        for word in words:\n",
    "            if word == kbt.sbj or word == kbt.obj:\n",
    "                feature_counter[word] += 1          \n",
    "            \n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_bag_of_words_featurizer(kbt, corpus, feature_counter):\n",
    "    subject_object_suffix = \"_SO\"\n",
    "    object_subject_suffix = \"_OS\"\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in words:\n",
    "            feature_counter[word + subject_object_suffix] += 1          \n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in words:\n",
    "            feature_counter[word + object_subject_suffix] += 1\n",
    "            \n",
    "    return feature_counter\n",
    "\n",
    "def directional_bag_of_bigrams_featurizer(kbt, corpus, feature_counter):\n",
    "    subject_object_suffix = \"_SO\"\n",
    "    object_subject_suffix = \"_OS\"\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in get_word_bigrams(words):            \n",
    "            feature_counter[word + subject_object_suffix] += 1\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in get_word_bigrams(words):            \n",
    "            feature_counter[word + subject_object_suffix] += 1\n",
    "            \n",
    "    return feature_counter\n",
    "\n",
    "def directional_bag_of_trigrams_featurizer(kbt, corpus, feature_counter):\n",
    "    subject_object_suffix = \"_SO\"\n",
    "    object_subject_suffix = \"_OS\"\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in get_word_trigrams(words):            \n",
    "            feature_counter[word + subject_object_suffix] += 1            \n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        words = ex.middle.split(' ')\n",
    "        for word in get_word_trigrams(words):            \n",
    "            feature_counter[word + subject_object_suffix] += 1\n",
    "            \n",
    "    return feature_counter\n",
    "\n",
    "def get_word_bigrams(words):\n",
    "    result = []\n",
    "    if len(words) > 0:\n",
    "        for index in range(len(words) - 1):\n",
    "            result.append(words[index] + ' ' + words[index + 1])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_word_trigrams(words):\n",
    "    result = []\n",
    "    if len(words) > 0:\n",
    "        for index in range(len(words) - 2):\n",
    "            result.append(words[index] + ' ' + words[index + 1] + ' ' + words[index + 2])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_bag_of_words_right_featurizer(kbt, corpus, feature_counter):\n",
    "    subject_object_suffix = \"_SO\"\n",
    "    object_subject_suffix = \"_OS\"\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.right.split(' '):            \n",
    "            feature_counter[word + subject_object_suffix] += 1 \n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.right.split(' '):\n",
    "            feature_counter[word + object_subject_suffix] += 1\n",
    "\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_bag_of_words_left_featurizer(kbt, corpus, feature_counter):\n",
    "    subject_object_suffix = \"_SO\"\n",
    "    object_subject_suffix = \"_OS\"\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.left.split(' '):            \n",
    "            feature_counter[word + subject_object_suffix] += 1 \n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        for word in ex.left.split(' '):\n",
    "            feature_counter[word + object_subject_suffix] += 1\n",
    "\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_length_featurizer(kbt, corpus, feature_counter):\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        feature_counter[str(len(ex.middle))] += 1 \n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        feature_counter[str(len(ex.middle))] += 1\n",
    "\n",
    "    return feature_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        tags = get_tags(ex.left_POS)\n",
    "        for tag in get_tag_bigrams(tags):\n",
    "            feature_counter[tag] += 1\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        tags = get_tags(ex.left_POS)\n",
    "        for tag in get_tag_bigrams(tags):\n",
    "            feature_counter[tag] += 1\n",
    "\n",
    "    return feature_counter\n",
    "\n",
    "def right_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        tags = get_tags(ex.right_POS)\n",
    "        for tag in get_tag_bigrams(tags):\n",
    "            feature_counter[tag] += 1\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        tags = get_tags(ex.right_POS)\n",
    "        for tag in get_tag_bigrams(tags):\n",
    "            feature_counter[tag] += 1\n",
    "\n",
    "    return feature_counter\n",
    "\n",
    "def middle_bigram_pos_tag_featurizer(kbt, corpus, feature_counter):\n",
    "\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        tags = get_tags(ex.middle_POS)\n",
    "        for tag in get_tag_bigrams(tags):\n",
    "            feature_counter[tag] += 1\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        tags = get_tags(ex.middle_POS)\n",
    "        for tag in get_tag_bigrams(tags):\n",
    "            feature_counter[tag] += 1\n",
    "\n",
    "    return feature_counter\n",
    "\n",
    "\n",
    "def get_tag_bigrams(tags):\n",
    "    \"\"\"Suggested helper method for `middle_bigram_pos_tag_featurizer`.\n",
    "    This should be defined so that it returns a list of str, where each\n",
    "    element is a POS bigram.\"\"\"\n",
    "    # The values of `start_symbol` and `end_symbol` are defined\n",
    "    # here so that you can use `test_middle_bigram_pos_tag_featurizer`.\n",
    "    start_symbol = \"<s>\"\n",
    "    end_symbol = \"</s>\"\n",
    "\n",
    "    ##### YOUR CODE HERE\n",
    "    result = []\n",
    "    if len(tags) > 0:\n",
    "        result = [start_symbol + ' ' + tags[0]]\n",
    "        for index in range(len(tags) - 1):\n",
    "            result.append(tags[index] + ' ' + tags[index + 1])\n",
    " \n",
    "        result.append(tags[len(tags) - 1] + ' ' + end_symbol)\n",
    "    \n",
    "    return result \n",
    "\n",
    "\n",
    "def get_tags(s):\n",
    "    \"\"\"Given a sequence of word/POS elements (lemmas), this function\n",
    "    returns a list containing just the POS elements, in order.\n",
    "    \"\"\"\n",
    "    return [parse_lem(lem)[1] for lem in s.strip().split(' ') if lem]\n",
    "\n",
    "\n",
    "def parse_lem(lem):\n",
    "    \"\"\"Helper method for parsing word/POS elements. It just splits\n",
    "    on the rightmost / and returns (word, POS) as a tuple of str.\"\"\"\n",
    "    return lem.strip().rsplit('/', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synset_featurizer(kbt, corpus, feature_counter):\n",
    "\n",
    "    ##### YOUR CODE HERE\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        synsets = get_synsets(ex.middle_POS)\n",
    "        for s in synsets:\n",
    "            feature_counter[s] += 1\n",
    "    \n",
    "    for ex in corpus.get_examples_for_entities(kbt.obj, kbt.sbj):\n",
    "        synsets = get_synsets(ex.middle_POS)\n",
    "        for s in synsets:\n",
    "            feature_counter[s] += 1\n",
    "\n",
    "    return feature_counter\n",
    "\n",
    "\n",
    "def get_synsets(s):\n",
    "    \"\"\"Suggested helper method for `synset_featurizer`. This should\n",
    "    be completed so that it returns a list of stringified Synsets\n",
    "    associated with elements of `s`.\n",
    "    \"\"\"\n",
    "    # Use `parse_lem` from the previous question to get a list of\n",
    "    # (word, POS) pairs. Remember to convert the POS strings.\n",
    "    wt = [parse_lem(lem) for lem in s.strip().split(' ') if lem]\n",
    "\n",
    "    synsets = []\n",
    "    ##### YOUR CODE HERE\n",
    "    for w, t in wt:\n",
    "        wnt = convert_tag(t)\n",
    "        wns = wn.synsets(w, wnt)\n",
    "        for s in wns:\n",
    "            synsets.append(str(s))\n",
    "\n",
    "    return synsets\n",
    "\n",
    "\n",
    "def convert_tag(t):\n",
    "    \"\"\"Converts tags so that they can be used by WordNet:\n",
    "\n",
    "    | Tag begins with | WordNet tag |\n",
    "    |-----------------|-------------|\n",
    "    | `N`             | `n`         |\n",
    "    | `V`             | `v`         |\n",
    "    | `J`             | `a`         |\n",
    "    | `R`             | `r`         |\n",
    "    | Otherwise       | `None`      |\n",
    "    \"\"\"\n",
    "    if t[0].lower() in {'n', 'v', 'r'}:\n",
    "        return t[0].lower()\n",
    "    elif t[0].lower() == 'j':\n",
    "        return 'a'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'xkcd': 2})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbt = rel_ext.KBTriple(rel='worked_at', sbj='Randall_Munroe', obj='xkcd')\n",
    "feature_counter = defaultdict(int)\n",
    "test = entity_mentions_featurizer(kbt, corpus, feature_counter)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1 - LogisticRegression\n",
    "\n",
    "*Note - removing stop words decreased the precision and recall.*\n",
    "\n",
    "*Note - adding features derived from words left to the object/subject and right to them, decreased the precision and recall too.*\n",
    "\n",
    "*Note - bigrams and trigrams based on the words between the two entity mentions decreased the performance of the model.*\n",
    "\n",
    "##### 1.a \n",
    "\n",
    "LogisticRegression model (solver='liblinear')\n",
    "\n",
    "with large 0.75 of total\n",
    "\n",
    "directional_bag_of_words_featurizer\n",
    "\n",
    "macro-average             0.764      0.356      0.605\n",
    "\n",
    "with tiny 0.05 of total\n",
    "\n",
    "directional_bag_of_words_featurizer\n",
    "\n",
    "macro-average             0.714      0.230      0.440 (with stop words)\n",
    "\n",
    "directional_bag_of_words_featurizer + middle_length_featurizer\n",
    "\n",
    "macro-average             0.696      0.216      0.450\n",
    "\n",
    "\n",
    "with tiny 0.15 of total\n",
    "\n",
    "directional_bag_of_words_featurizer,\n",
    "                                middle_bigram_pos_tag_featurizer,\n",
    "                                synset_featurizer,\n",
    "                                middle_length_featurizer\n",
    "\n",
    "macro-average             0.730      0.354      0.579\n",
    "\n",
    "\n",
    "with train 0.75 of total\n",
    "\n",
    "middle_bigram_pos_tag_featurizer + directional_bag_of_words_featurizer + synset_featurizer\n",
    "\n",
    "macro-average             0.738      0.417      0.627\n",
    "\n",
    "with tiny 0.05 of total\n",
    "\n",
    "directional_bag_of_words + middle_bigram_pos_tag_featurizer + synset_featurizer\n",
    "\n",
    "macro-average             0.702      0.266      0.503\n",
    "\n",
    "directional_bag_of_words + middle_bigram_pos_tag_featurizer\n",
    "\n",
    "macro-average             0.764      0.243      0.497\n",
    "\n",
    "directional_bag_of_words + directional_bigrams_of_words\n",
    "\n",
    "macro-average             0.747      0.230      0.474\n",
    "\n",
    "directional_bag_of_words + bigrams_of_words\n",
    "\n",
    "macro-average             0.795      0.218      0.465\n",
    "\n",
    "bigrams_of_words_featurizer\n",
    "\n",
    "macro-average             0.794      0.173      0.396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factory = lambda: LogisticRegression(\n",
    "    fit_intercept=True, solver='liblinear', max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.840      0.403      0.691        340       5716\n",
      "author                    0.833      0.656      0.790        509       5885\n",
      "capital                   0.633      0.200      0.442         95       5471\n",
      "contains                  0.769      0.772      0.770       3904       9280\n",
      "film_performance          0.818      0.706      0.793        766       6142\n",
      "founders                  0.709      0.442      0.633        380       5756\n",
      "genre                     0.701      0.400      0.609        170       5546\n",
      "has_sibling               0.877      0.273      0.608        499       5875\n",
      "has_spouse                0.905      0.367      0.700        594       5970\n",
      "is_a                      0.682      0.350      0.574        497       5873\n",
      "nationality               0.579      0.279      0.477        301       5677\n",
      "parents                   0.828      0.571      0.759        312       5688\n",
      "place_of_birth            0.649      0.262      0.501        233       5609\n",
      "place_of_death            0.443      0.170      0.335        159       5535\n",
      "profession                0.716      0.336      0.584        247       5623\n",
      "worked_at                 0.721      0.331      0.583        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.731      0.407      0.615       9248      95264\n"
     ]
    }
   ],
   "source": [
    "lr_results = rel_ext.experiment(splits, train_split='train', test_split='dev', \n",
    "                                featurizers=[directional_bag_of_words_featurizer,\n",
    "                                             middle_bigram_pos_tag_featurizer,\n",
    "                                             synset_featurizer,\n",
    "                                             middle_length_featurizer], \n",
    "                                model_factory=model_factory,\n",
    "                                verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation adjoins:\n",
      "\n",
      "     1.593 into_SO\n",
      "     1.539 Córdoba_OS\n",
      "     1.489 Valais_OS\n",
      "     ..... .....\n",
      "    -1.099 46\n",
      "    -1.113 for_SO\n",
      "    -1.200 <s> NN\n",
      "\n",
      "Highest and lowest feature weights for relation author:\n",
      "\n",
      "     2.715 by_SO\n",
      "     1.820 Dossie_SO\n",
      "     1.808 Easton_SO\n",
      "     ..... .....\n",
      "    -1.662 <s> TO\n",
      "    -2.060 's_SO\n",
      "    -2.551 by_OS\n",
      "\n",
      "Highest and lowest feature weights for relation capital:\n",
      "\n",
      "     1.186 including_SO\n",
      "     1.112 Synset('airport.n.01')\n",
      "     1.067 Tortola_OS\n",
      "     ..... .....\n",
      "    -0.992 . </s>\n",
      "    -1.197 CC </s>\n",
      "    -1.495 Glamorgan_OS\n",
      "\n",
      "Highest and lowest feature weights for relation contains:\n",
      "\n",
      "     2.648 third-largest_SO\n",
      "     2.404 Ceuta_SO\n",
      "     2.187 after_SO\n",
      "     ..... .....\n",
      "    -2.800 peninsula_OS\n",
      "    -5.553 Bronx_OS\n",
      "    -5.553 Synset('bronx.n.01')\n",
      "\n",
      "Highest and lowest feature weights for relation film_performance:\n",
      "\n",
      "     2.740 with_OS\n",
      "     2.552 Synset('movie.n.01')\n",
      "     2.549 alongside_OS\n",
      "     ..... .....\n",
      "    -1.922 film_OS\n",
      "    -1.980 stars_SO\n",
      "    -2.193 starring_SO\n",
      "\n",
      "Highest and lowest feature weights for relation founders:\n",
      "\n",
      "     2.859 co-founder_OS\n",
      "     2.632 co-founder_SO\n",
      "     2.151 Synset('president.n.04')\n",
      "     ..... .....\n",
      "    -1.419 , </s>\n",
      "    -1.482 CD </s>\n",
      "    -1.486 the_SO\n",
      "\n",
      "Highest and lowest feature weights for relation genre:\n",
      "\n",
      "     2.385 movie_OS\n",
      "     2.254 Since_OS\n",
      "     1.618 series_OS\n",
      "     ..... .....\n",
      "    -1.602 <s> CC\n",
      "    -1.913 <s> POS\n",
      "    -2.027 show_SO\n",
      "\n",
      "Highest and lowest feature weights for relation has_sibling:\n",
      "\n",
      "     2.500 Synset('nephew.n.01')\n",
      "     2.049 Synset('stepbrother.n.01')\n",
      "     1.993 Singh_SO\n",
      "     ..... .....\n",
      "    -1.231 24\n",
      "    -1.752 <s> NN\n",
      "    -1.935 DT </s>\n",
      "\n",
      "Highest and lowest feature weights for relation has_spouse:\n",
      "\n",
      "     3.871 Synset('wife.n.01')\n",
      "     3.427 Synset('husband.n.01')\n",
      "     2.915 Synset('widow.n.01')\n",
      "     ..... .....\n",
      "    -1.335 <s> :\n",
      "    -1.365 <s> NN\n",
      "    -1.993 DT </s>\n",
      "\n",
      "Highest and lowest feature weights for relation is_a:\n",
      "\n",
      "     2.003 Synset('suborder.n.01')\n",
      "     1.990 such_OS\n",
      "     1.887 _OS\n",
      "     ..... .....\n",
      "    -1.870 <s> POS\n",
      "    -2.481 characin_OS\n",
      "    -2.895 Synset('characin.n.01')\n",
      "\n",
      "Highest and lowest feature weights for relation nationality:\n",
      "\n",
      "     2.137 of_SO\n",
      "     2.040 from_SO\n",
      "     1.866 Southern_SO\n",
      "     ..... .....\n",
      "    -1.517 with_SO\n",
      "    -1.554 CC </s>\n",
      "    -1.565 <s> CC\n",
      "\n",
      "Highest and lowest feature weights for relation parents:\n",
      "\n",
      "     3.243 Synset('daughter.n.01')\n",
      "     2.399 Synset('son.n.02')\n",
      "     2.255 Synset('wife.n.01')\n",
      "     ..... .....\n",
      "    -1.200 <s> NN\n",
      "    -1.386 ‘_SO\n",
      "    -2.184 DT </s>\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_birth:\n",
      "\n",
      "     2.393 Synset('mayor.n.01')\n",
      "     2.197 from_SO\n",
      "     1.908 1885_OS\n",
      "     ..... .....\n",
      "    -1.413 <s> VBG\n",
      "    -1.417 , </s>\n",
      "    -1.791 CC </s>\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_death:\n",
      "\n",
      "     1.747 Lineal_SO\n",
      "     1.747 Ciudad_SO\n",
      "     1.673 where_OS\n",
      "     ..... .....\n",
      "    -1.486 3\n",
      "    -1.694 CC </s>\n",
      "    -1.890 1\n",
      "\n",
      "Highest and lowest feature weights for relation profession:\n",
      "\n",
      "     2.403 _OS\n",
      "     1.695 American_SO\n",
      "     1.663 0\n",
      "     ..... .....\n",
      "    -1.359 a_OS\n",
      "    -1.360 becomes_SO\n",
      "    -1.520 NN -RRB-\n",
      "\n",
      "Highest and lowest feature weights for relation worked_at:\n",
      "\n",
      "     2.429 Synset('professor.n.01')\n",
      "     1.996 Synset('chief_executive_officer.n.01')\n",
      "     1.878 Co-Founder_SO\n",
      "     ..... .....\n",
      "    -1.214 CC </s>\n",
      "    -1.313 , </s>\n",
      "    -1.513 at_OS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(lr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 2 SVC with RBF \n",
    "\n",
    "with train 0.75 of total\n",
    "\n",
    "middle_bigram_pos_tag_featurizer + directional_bag_of_words_featurizer + synset_featurizer\n",
    "\n",
    "macro-average             0.908      0.264      0.594\n",
    "\n",
    "directional_bag_of_words_featurizer\n",
    "\n",
    "macro-average             0.815      0.274      0.573\n",
    "\n",
    "with tiny 0.05 of total\n",
    "\n",
    "middle_bigram_pos_tag_featurizer + directional_bag_of_words_featurizer + synset_featurizer\n",
    "\n",
    "macro-average             0.910      0.223      0.542\n",
    "\n",
    "directional_bag_of_words_featurizer\n",
    "\n",
    "macro-average             0.624      0.234      0.457\n",
    "\n",
    "\n",
    "linear with tiny 0.15 of total\n",
    "\n",
    "middle_bigram_pos_tag_featurizer,\n",
    "                               directional_bag_of_words_featurizer,\n",
    "                               synset_featurizer,\n",
    "                               middle_length_featurizer\n",
    "\n",
    "macro-average             0.577      0.463      0.536\n",
    "\n",
    "RBF with tiny 0.15 of total\n",
    "\n",
    "middle_bigram_pos_tag_featurizer,\n",
    "                               directional_bag_of_words_featurizer,\n",
    "                               synset_featurizer,\n",
    "                               middle_length_featurizer\n",
    "\n",
    "macro-average             0.911      0.231      0.560\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcl_model_factory = lambda: SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.692      0.403      0.605        340       5716\n",
      "author                    0.730      0.680      0.719        509       5885\n",
      "capital                   0.491      0.274      0.423         95       5471\n",
      "contains                  0.754      0.833      0.769       3904       9280\n",
      "film_performance          0.740      0.709      0.733        766       6142\n",
      "founders                  0.583      0.497      0.564        380       5756\n",
      "genre                     0.573      0.506      0.558        170       5546\n",
      "has_sibling               0.720      0.263      0.534        499       5875\n",
      "has_spouse                0.753      0.365      0.621        594       5970\n",
      "is_a                      0.535      0.380      0.495        497       5873\n",
      "nationality               0.433      0.352      0.414        301       5677\n",
      "parents                   0.716      0.574      0.682        312       5688\n",
      "place_of_birth            0.442      0.313      0.409        233       5609\n",
      "place_of_death            0.344      0.195      0.299        159       5535\n",
      "profession                0.534      0.381      0.494        247       5623\n",
      "worked_at                 0.633      0.393      0.564        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.605      0.445      0.555       9248      95264\n"
     ]
    }
   ],
   "source": [
    "svcl_results = rel_ext.experiment(splits, train_split='train', test_split='dev', \n",
    "                                  featurizers=[middle_bigram_pos_tag_featurizer,\n",
    "                                               directional_bag_of_words_featurizer,\n",
    "                                               synset_featurizer,\n",
    "                                               middle_length_featurizer], \n",
    "                                  model_factory=svcl_model_factory,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest and lowest feature weights for relation adjoins:\n",
      "\n",
      "     1.435 Valais_OS\n",
      "     1.409 Córdoba_OS\n",
      "     1.382 Synset('nearby.s.01')\n",
      "     ..... .....\n",
      "    -0.778 , VBZ\n",
      "    -0.895 Baffin_SO\n",
      "    -1.000 Tyrone_SO\n",
      "\n",
      "Highest and lowest feature weights for relation author:\n",
      "\n",
      "     1.390 1920_OS\n",
      "     1.342 by_SO\n",
      "     1.227 '_OS\n",
      "     ..... .....\n",
      "    -1.334 1985_OS\n",
      "    -1.447 's_SO\n",
      "    -1.776 by_OS\n",
      "\n",
      "Highest and lowest feature weights for relation capital:\n",
      "\n",
      "     1.000 Lenzie_OS\n",
      "     1.000 Golmud_OS\n",
      "     0.981 burgraviates_SO\n",
      "     ..... .....\n",
      "    -0.673 NN -RRB-\n",
      "    -0.711 Patna_SO\n",
      "    -0.969 Glamorgan_OS\n",
      "\n",
      "Highest and lowest feature weights for relation contains:\n",
      "\n",
      "     2.963 third-largest_SO\n",
      "     2.488 Ceuta_SO\n",
      "     2.022 Gozo_SO\n",
      "     ..... .....\n",
      "    -2.413 peninsula_OS\n",
      "    -3.842 Synset('bronx.n.01')\n",
      "    -3.842 Bronx_OS\n",
      "\n",
      "Highest and lowest feature weights for relation film_performance:\n",
      "\n",
      "     1.802 with_OS\n",
      "     1.674 alongside_OS\n",
      "     1.423 co-star_OS\n",
      "     ..... .....\n",
      "    -1.350 CC VBG\n",
      "    -1.474 stars_SO\n",
      "    -1.636 Dhanush_OS\n",
      "\n",
      "Highest and lowest feature weights for relation founders:\n",
      "\n",
      "     1.801 co-founder_SO\n",
      "     1.639 co-founder_OS\n",
      "     1.453 co-founded_OS\n",
      "     ..... .....\n",
      "    -1.000 -led_SO\n",
      "    -1.047 POS ``\n",
      "    -1.106 photographer_SO\n",
      "\n",
      "Highest and lowest feature weights for relation genre:\n",
      "\n",
      "     1.568 Since_OS\n",
      "     1.515 movie_OS\n",
      "     1.106 program_OS\n",
      "     ..... .....\n",
      "    -1.000 1964_SO\n",
      "    -1.001 blockbuster_OS\n",
      "    -1.410 show_SO\n",
      "\n",
      "Highest and lowest feature weights for relation has_sibling:\n",
      "\n",
      "     1.818 Kermit_SO\n",
      "     1.633 Marlon_SO\n",
      "     1.420 Synset('stepbrother.n.01')\n",
      "     ..... .....\n",
      "    -1.000 Baden-Powell_SO\n",
      "    -1.000 1945–2003_SO\n",
      "    -1.041 Tamela_SO\n",
      "\n",
      "Highest and lowest feature weights for relation has_spouse:\n",
      "\n",
      "     1.759 Synset('wife.n.01')\n",
      "     1.601 Synset('husband.n.01')\n",
      "     1.491 Kripke_OS\n",
      "     ..... .....\n",
      "    -1.000 Murthy_SO\n",
      "    -1.000 1926_SO\n",
      "    -1.112 Ann_SO\n",
      "\n",
      "Highest and lowest feature weights for relation is_a:\n",
      "\n",
      "     1.541 Synset('suborder.n.01')\n",
      "     1.422 , JJS\n",
      "     1.353 Kamen_SO\n",
      "     ..... .....\n",
      "    -1.155 becomes_SO\n",
      "    -2.108 characin_OS\n",
      "    -2.459 Synset('characin.n.01')\n",
      "\n",
      "Highest and lowest feature weights for relation nationality:\n",
      "\n",
      "     1.480 Synset('caliph.n.01')\n",
      "     1.285 of_SO\n",
      "     1.284 Filming_SO\n",
      "     ..... .....\n",
      "    -1.000 1896_SO\n",
      "    -1.000 1867_SO\n",
      "    -1.015 Synset('drummer.n.01')\n",
      "\n",
      "Highest and lowest feature weights for relation parents:\n",
      "\n",
      "     1.404 Synset('daughter.n.01')\n",
      "     1.345 grandmother_OS\n",
      "     1.333 grandfather_SO\n",
      "     ..... .....\n",
      "    -1.000 1847-1928_SO\n",
      "    -1.179 ‘_SO\n",
      "    -1.398 Jahangir_SO\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_birth:\n",
      "\n",
      "     1.897 1885_OS\n",
      "     1.542 Synset('mayor.n.01')\n",
      "     1.103 from_SO\n",
      "     ..... .....\n",
      "    -1.000 Sorrento_OS\n",
      "    -1.000 1896_SO\n",
      "    -1.010 -born_SO\n",
      "\n",
      "Highest and lowest feature weights for relation place_of_death:\n",
      "\n",
      "     1.000 revolt_OS\n",
      "     1.000 rebuilt_SO\n",
      "     1.000 businesswoman_OS\n",
      "     ..... .....\n",
      "    -0.907 WLUP_SO\n",
      "    -1.000 1982_SO\n",
      "    -1.000 1896_SO\n",
      "\n",
      "Highest and lowest feature weights for relation profession:\n",
      "\n",
      "     1.366 Kamen_SO\n",
      "     1.333 _OS\n",
      "     1.216 1971-_SO\n",
      "     ..... .....\n",
      "    -1.000 cumbia_SO\n",
      "    -1.000 conjunto_SO\n",
      "    -1.072 becomes_SO\n",
      "\n",
      "Highest and lowest feature weights for relation worked_at:\n",
      "\n",
      "     1.483 Co-Founder_SO\n",
      "     1.156 superstar_OS\n",
      "     1.132 Synset('chief_executive_officer.n.01')\n",
      "     ..... .....\n",
      "    -0.943 start_SO\n",
      "    -1.000 Wikia_OS\n",
      "    -1.263 headed_SO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rel_ext.examine_model_weights(svcl_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "svcr_model_factory = lambda: SVC(gamma=2, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   1.000      0.171      0.507        340       5716\n",
      "author                    0.903      0.346      0.683        509       5885\n",
      "capital                   0.783      0.189      0.481         95       5471\n",
      "contains                  0.757      0.346      0.612       3904       9280\n",
      "film_performance          0.816      0.168      0.461        766       6142\n",
      "founders                  0.859      0.208      0.528        380       5756\n",
      "genre                     0.737      0.165      0.435        170       5546\n",
      "has_sibling               0.921      0.305      0.656        499       5875\n",
      "has_spouse                0.932      0.345      0.695        594       5970\n",
      "is_a                      0.898      0.266      0.608        497       5873\n",
      "nationality               0.921      0.462      0.768        301       5677\n",
      "parents                   0.989      0.288      0.666        312       5688\n",
      "place_of_birth            0.985      0.275      0.649        233       5609\n",
      "place_of_death            0.935      0.270      0.627        159       5535\n",
      "profession                0.850      0.275      0.600        247       5623\n",
      "worked_at                 0.867      0.161      0.462        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.884      0.265      0.590       9248      95264\n"
     ]
    }
   ],
   "source": [
    "svcr_results = rel_ext.experiment(splits, train_split='train', test_split='dev', \n",
    "                                  featurizers=[middle_bigram_pos_tag_featurizer,\n",
    "                                               directional_bag_of_words_featurizer,\n",
    "                                               synset_featurizer,\n",
    "                                               middle_length_featurizer], \n",
    "                                  model_factory=svcr_model_factory,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 3\n",
    "\n",
    "k-nearest (default parameters) and directional_bag_of_words_featurizer\n",
    "\n",
    "macro-average             0.352      0.437      0.338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc_model_factory = lambda: KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.455      0.526      0.468        340       5716\n",
      "author                    0.563      0.764      0.594        509       5885\n",
      "capital                   0.240      0.463      0.266         95       5471\n",
      "contains                  0.741      0.741      0.741       3904       9280\n",
      "film_performance          0.601      0.658      0.611        766       6142\n",
      "founders                  0.416      0.555      0.438        380       5756\n",
      "genre                     0.404      0.482      0.418        170       5546\n",
      "has_sibling               0.456      0.543      0.471        499       5875\n",
      "has_spouse                0.521      0.613      0.537        594       5970\n",
      "is_a                      0.402      0.555      0.426        497       5873\n",
      "nationality               0.409      0.638      0.441        301       5677\n",
      "parents                   0.434      0.622      0.462        312       5688\n",
      "place_of_birth            0.333      0.515      0.359        233       5609\n",
      "place_of_death            0.204      0.371      0.224        159       5535\n",
      "profession                0.315      0.563      0.346        247       5623\n",
      "worked_at                 0.315      0.421      0.332        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.426      0.564      0.446       9248      95264\n"
     ]
    }
   ],
   "source": [
    "knc_results = rel_ext.experiment(splits, train_split='train', test_split='dev', \n",
    "                                 featurizers=[middle_bigram_pos_tag_featurizer,\n",
    "                                              directional_bag_of_words_featurizer,\n",
    "                                              synset_featurizer,\n",
    "                                              middle_length_featurizer], \n",
    "                                 model_factory=knc_model_factory,\n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment 4 Multi-layer perception classifier\n",
    "\n",
    "with train dataset 0.75 of total\n",
    "\n",
    "directional_bag_of_words_featurizer with learning_rate_init=0.001\n",
    "\n",
    "macro-average             0.785      0.396      0.637 (with alpha=0.17)\n",
    "\n",
    "\n",
    "with tiny dataset 0.05 or total\n",
    "\n",
    "middle_bigram_pos_tag_featurizer, directional_bag_of_words_featurizer, synset_featurizer\n",
    "\n",
    "macro-average             0.702      0.459      0.620\n",
    "\n",
    "\n",
    "directional_bag_of_words_featurizer with learning_rate_init=0.001\n",
    "\n",
    "macro-average             0.649      0.364      0.522 (with alpha=0.17)\n",
    "\n",
    "macro-average             0.649      0.355      0.517 (with alpha=0.18)\n",
    "\n",
    "macro-average             0.658      0.339      0.517 (with alpha=0.2)\n",
    "\n",
    "macro-average             0.644      0.357      0.516 (with alpha=0.15)\n",
    "\n",
    "macro-average             0.578      0.419      0.511 (with alpha=0.1)\n",
    "\n",
    "macro-average             0.703      0.273      0.502 (with alpha=0.3)\n",
    "\n",
    "macro-average             0.528      0.451      0.495 (with alpha=0.05)\n",
    "\n",
    "macro-average             0.742      0.249      0.490 (with alpha=0.5)\n",
    "\n",
    "\n",
    "with learning_rate_init=0.1\n",
    "\n",
    "directional_bag_of_words_featurizer\n",
    "\n",
    "macro-average             0.615      0.366      0.500 (with alpha=0.2)\n",
    "\n",
    "directional_bag_of_words_featurizer, middle_length_featurizer\n",
    "\n",
    "macro-average             0.645      0.411      0.550 (with alpha=0.2)\n",
    "\n",
    "directional_bag_of_words_featurizer, middle_length_featurizer, \n",
    "middle_bigram_pos_tag_featurizer\n",
    "\n",
    "macro-average             0.656      0.417      0.566 (with alpha=0.2)\n",
    "\n",
    "directional_bag_of_words_featurizer, middle_length_featurizer, middle_bigram_pos_tag_featurizer, synset_featurizer\n",
    "\n",
    "macro-average             0.653      0.416      0.554 (with alpha=0.2)\n",
    "\n",
    "with tiny dataset 0.15 or total and learning rate of 0.1\n",
    "\n",
    "directional_bag_of_words_featurizer, middle_length_featurizer, middle_bigram_pos_tag_featurizer\n",
    "\n",
    "macro-average             0.764      0.446      0.642 (with alpha=0.2)\n",
    "\n",
    "with learning_rate_init=0.01\n",
    "\n",
    "directional_bag_of_words_featurizer, middle_length_featurizer, \n",
    "middle_bigram_pos_tag_featurizer\n",
    "\n",
    "macro-average             0.648      0.458      0.584 \n",
    "\n",
    "directional_bag_of_words_featurizer \n",
    "\n",
    "macro-average             0.622      0.344      0.510 (with alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_HOME = os.path.join('data', 'glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_middle_featurizer(kbt, corpus, np_func=np.sum):\n",
    "    reps = []\n",
    "    for ex in corpus.get_examples_for_entities(kbt.sbj, kbt.obj):\n",
    "        for word in ex.middle.split():\n",
    "            rep = glove_lookup.get(word)\n",
    "            if rep is not None:\n",
    "                reps.append(rep)                        \n",
    "    # A random representation of the right dimensionality if the\n",
    "    # example happens not to overlap with GloVe's vocabulary:\n",
    "    if len(reps) == 0:\n",
    "        dim = len(next(iter(glove_lookup.values())))\n",
    "        return utils.randvec(n=dim)\n",
    "    else:\n",
    "        return np_func(reps, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_model_factory = lambda: MLPClassifier(\n",
    "    alpha=0.05, learning_rate_init=0.01, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.822      0.409      0.684        340       5716\n",
      "author                    0.595      0.462      0.562        509       5885\n",
      "capital                   0.378      0.295      0.358         95       5471\n",
      "contains                  0.592      0.594      0.592       3904       9280\n",
      "film_performance          0.528      0.415      0.501        766       6142\n",
      "founders                  0.502      0.292      0.439        380       5756\n",
      "genre                     0.291      0.094      0.205        170       5546\n",
      "has_sibling               0.542      0.246      0.437        499       5875\n",
      "has_spouse                0.542      0.263      0.447        594       5970\n",
      "is_a                      0.345      0.245      0.319        497       5873\n",
      "nationality               0.526      0.302      0.458        301       5677\n",
      "parents                   0.632      0.391      0.563        312       5688\n",
      "place_of_birth            0.444      0.206      0.361        233       5609\n",
      "place_of_death            0.367      0.138      0.276        159       5535\n",
      "profession                0.408      0.243      0.359        247       5623\n",
      "worked_at                 0.667      0.281      0.523        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.511      0.305      0.443       9248      95264\n"
     ]
    }
   ],
   "source": [
    "mlpc_glv_results = rel_ext.experiment(splits, train_split='train', test_split='dev', \n",
    "                                      featurizers=[glove_middle_featurizer], \n",
    "                                      model_factory=mlpc_model_factory,\n",
    "                                      vectorize=False,\n",
    "                                      verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_model_factory = lambda: MLPClassifier(\n",
    "    alpha=0.1, learning_rate_init=0.01, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.906      0.594      0.820        340       5716\n",
      "author                    0.835      0.825      0.833        509       5885\n",
      "capital                   0.638      0.316      0.530         95       5471\n",
      "contains                  0.779      0.858      0.794       3904       9280\n",
      "film_performance          0.833      0.676      0.796        766       6142\n",
      "founders                  0.740      0.650      0.720        380       5756\n",
      "genre                     0.717      0.447      0.640        170       5546\n",
      "has_sibling               0.785      0.521      0.713        499       5875\n",
      "has_spouse                0.848      0.678      0.808        594       5970\n",
      "is_a                      0.752      0.507      0.686        497       5873\n",
      "nationality               0.756      0.668      0.736        301       5677\n",
      "parents                   0.879      0.747      0.849        312       5688\n",
      "place_of_birth            0.846      0.494      0.740        233       5609\n",
      "place_of_death            0.716      0.491      0.655        159       5535\n",
      "profession                0.814      0.530      0.735        247       5623\n",
      "worked_at                 0.694      0.355      0.583        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.784      0.585      0.727       9248      95264\n"
     ]
    }
   ],
   "source": [
    "mlpc_results = rel_ext.experiment(splits, train_split='train', test_split='dev', \n",
    "                                  featurizers=[directional_bag_of_words_featurizer,\n",
    "                                               middle_length_featurizer,\n",
    "                                               middle_bigram_pos_tag_featurizer], \n",
    "                                  model_factory=mlpc_model_factory,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = mlpc_results['models']['adjoins'].coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_weights = sorted([(wgt, idx) for idx, wgt in enumerate(coefs[1])], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model_factory = lambda: SGDClassifier(alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.823      0.438      0.700        340       5716\n",
      "author                    0.817      0.666      0.781        509       5885\n",
      "capital                   0.519      0.295      0.450         95       5471\n",
      "contains                  0.777      0.768      0.775       3904       9280\n",
      "film_performance          0.783      0.714      0.768        766       6142\n",
      "founders                  0.699      0.439      0.625        380       5756\n",
      "genre                     0.598      0.429      0.555        170       5546\n",
      "has_sibling               0.746      0.265      0.547        499       5875\n",
      "has_spouse                0.842      0.387      0.682        594       5970\n",
      "is_a                      0.557      0.256      0.451        497       5873\n",
      "nationality               0.516      0.272      0.438        301       5677\n",
      "parents                   0.749      0.554      0.700        312       5688\n",
      "place_of_birth            0.722      0.245      0.519        233       5609\n",
      "place_of_death            0.453      0.214      0.370        159       5535\n",
      "profession                0.743      0.340      0.601        247       5623\n",
      "worked_at                 0.556      0.368      0.505        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.681      0.416      0.592       9248      95264\n"
     ]
    }
   ],
   "source": [
    "sgd_results = rel_ext.experiment(splits, train_split='train', test_split='dev', \n",
    "                                  featurizers=[directional_bag_of_words_featurizer,\n",
    "                                               middle_length_featurizer,\n",
    "                                               middle_bigram_pos_tag_featurizer], \n",
    "                                  model_factory=sgd_model_factory,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation              precision     recall    f-score    support       size\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "adjoins                   0.893      0.468      0.756        340       5716\n",
      "author                    0.654      0.458      0.603        509       5885\n",
      "capital                   0.299      0.242      0.285         95       5471\n",
      "contains                  0.595      0.442      0.556       3904       9280\n",
      "film_performance          0.728      0.325      0.583        766       6142\n",
      "founders                  0.570      0.192      0.409        380       5756\n",
      "genre                     0.429      0.088      0.242        170       5546\n",
      "has_sibling               0.658      0.154      0.398        499       5875\n",
      "has_spouse                0.715      0.364      0.599        594       5970\n",
      "is_a                      0.562      0.119      0.322        497       5873\n",
      "nationality               0.644      0.186      0.431        301       5677\n",
      "parents                   0.872      0.349      0.671        312       5688\n",
      "place_of_birth            0.500      0.073      0.230        233       5609\n",
      "place_of_death            0.239      0.107      0.192        159       5535\n",
      "profession                0.545      0.194      0.401        247       5623\n",
      "worked_at                 0.559      0.351      0.500        242       5618\n",
      "------------------    ---------  ---------  ---------  ---------  ---------\n",
      "macro-average             0.591      0.257      0.449       9248      95264\n"
     ]
    }
   ],
   "source": [
    "sgd_glv_results = rel_ext.experiment(splits, train_split='train', test_split='dev', \n",
    "                                  featurizers=[glove_middle_featurizer], \n",
    "                                     model_factory=sgd_model_factory,\n",
    "                                     vectorize=False,\n",
    "                                     verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
