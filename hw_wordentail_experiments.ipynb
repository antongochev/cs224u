{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "import nli\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch_model_base import TorchModelBase\n",
    "from torch_rnn_classifier import TorchRNNClassifier, TorchRNNModel\n",
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import utils\n",
    "import warnings\n",
    "import json\n",
    "import nli_ext\n",
    "from gensim import models\n",
    "import gensim.downloader as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fix_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_HOME = os.path.join('data', 'glove.6B')\n",
    "\n",
    "DATA_HOME = os.path.join(\"data\", \"nlidata\")\n",
    "\n",
    "SNLI_HOME = os.path.join(DATA_HOME, \"snli_1.0\")\n",
    "\n",
    "MULTINLI_HOME = os.path.join(DATA_HOME, \"multinli_1.0\")\n",
    "\n",
    "ANNOTATIONS_HOME = os.path.join(DATA_HOME, \"multinli_1.0_annotations\")\n",
    "\n",
    "wordentail_filename = os.path.join(\n",
    "    DATA_HOME, 'nli_wordentail_bakeoff_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wordentail_filename) as f:\n",
    "    wordentail_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wn.synsets('house', 'n')[0].name().split('.n')[0]\n",
    "\n",
    "def enhance_examples_with_synonyms():\n",
    "    updated_wordentail_train_data = list()\n",
    "    for pair in wordentail_data['train']:\n",
    "        words, relation = pair\n",
    "        updated_wordentail_train_data.append(pair)\n",
    "        if relation == 1:            \n",
    "            syn = wn.synsets(words[0], 'n')\n",
    "            if len(syn) > 0:\n",
    "                for w in syn:\n",
    "                    updated_wordentail_train_data.append(\n",
    "                        [[words[1], w.name().split('.n')[0]], relation]\n",
    "                    )\n",
    "\n",
    "            syn = wn.synsets(words[1], 'n')\n",
    "            if len(syn) > 0:\n",
    "                for w in syn:\n",
    "                    updated_wordentail_train_data.append(\n",
    "                        [[w.name().split('.n')[0], words[0]], relation]\n",
    "                    )\n",
    "    return updated_wordentail_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_wordentail_train_data = enhance_examples_with_synonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52928"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_wordentail_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randvec(w, n=50, lower=-1.0, upper=1.0):\n",
    "    \"\"\"Returns a random vector of length `n`. `w` is ignored.\"\"\"\n",
    "    return utils.randvec(n=n, lower=lower, upper=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(path):\n",
    "    # Creates a dict mapping strings (words) to GloVe vectors:\n",
    "    glove = utils.glove2dict(path)\n",
    "    # Remove stop words\n",
    "#     result = dict()\n",
    "#     for w in glove.keys():\n",
    "#         if w not in spacy_stopwords:\n",
    "#             result[w] = glove[w]\n",
    "    return glove\n",
    "\n",
    "def load_glove50():\n",
    "    return load_glove(\n",
    "        os.path.join(GLOVE_HOME, 'glove.6B.50d.txt'))\n",
    "\n",
    "def load_glove100():\n",
    "    return load_glove(\n",
    "        os.path.join(GLOVE_HOME, 'glove.6B.100d.txt'))\n",
    "\n",
    "def load_glove200():\n",
    "    return load_glove(\n",
    "        os.path.join(GLOVE_HOME, 'glove.6B.200d.txt'))\n",
    "\n",
    "def load_glove300():\n",
    "    return load_glove(\n",
    "        os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     glove200 = load_glove200()\n",
    "\n",
    "#     def glove_vec200(w):\n",
    "#         return glove200.get(w, randvec(w, n=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tje GloVe data as a dictionannary of vectors\n",
    "glove50 = load_glove50()\n",
    "glove100 = load_glove100()\n",
    "glove200 = load_glove200()\n",
    "glove300 = load_glove300()\n",
    "\n",
    "def glove_vec50(w):\n",
    "    return glove50.get(w, randvec(w, n=50))\n",
    "\n",
    "def glove_vec100(w):\n",
    "    return glove100.get(w, randvec(w, n=100))\n",
    "\n",
    "def glove_vec200(w):\n",
    "    return glove200.get(w, randvec(w, n=200))\n",
    "\n",
    "def glove_vec300(w):\n",
    "    return glove300.get(w, randvec(w, n=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews_path = os.path.join('data', 'GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = models.KeyedVectors.load_word2vec_format(\n",
    "    gnews_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec300(w):\n",
    "    result = randvec(w, n=300)\n",
    "    try:\n",
    "        result = word2vec[w]\n",
    "    except KeyError:    \n",
    "        return result\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_combined(w):\n",
    "    result = glove300.get(w)\n",
    "    if result is not None:\n",
    "        return result\n",
    "    \n",
    "    result = randvec(w, n=300)\n",
    "    try:\n",
    "        result = word2vec[w]\n",
    "    except KeyError:    \n",
    "        return result\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-------------------------------------------------] 3.3% 54.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==------------------------------------------------] 5.1% 85.4/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===-----------------------------------------------] 7.7% 127.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====----------------------------------------------] 9.8% 163.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 11.7% 194.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======-------------------------------------------] 14.2% 236.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========-----------------------------------------] 18.2% 303.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========----------------------------------------] 20.7% 344.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 23.2% 385.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============--------------------------------------] 25.5% 424.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============-------------------------------------] 28.0% 464.9/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============-----------------------------------] 30.2% 501.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================----------------------------------] 32.3% 537.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================---------------------------------] 34.3% 570.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================--------------------------------] 36.6% 608.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 39.4% 655.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================-----------------------------] 42.0% 699.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================----------------------------] 44.0% 732.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================---------------------------] 46.1% 766.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================--------------------------] 48.6% 807.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================-------------------------] 51.0% 847.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 53.7% 892.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================-----------------------] 55.9% 929.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================---------------------] 58.5% 972.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================--------------------] 61.0% 1013.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================-------------------] 63.4% 1054.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================================------------------] 65.6% 1090.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================-----------------] 68.0% 1130.2/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================================---------------] 70.8% 1177.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 73.1% 1215.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================================-------------] 75.4% 1253.7/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================------------] 77.8% 1294.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========================================----------] 80.5% 1338.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================================---------] 83.9% 1395.5/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========================================-------] 86.2% 1434.1/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================------] 88.4% 1470.6/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================================-----] 91.3% 1517.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================================----] 93.5% 1554.8/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================================---] 96.0% 1596.0/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================-] 98.2% 1633.3/1662.8MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "path = gd.load('word2vec-google-news-300', return_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd.BASE_DIR = os.path.join('data', 'gensim-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter(w):\n",
    "    result = randvec(w, n=200)\n",
    "    try:\n",
    "        result = glove_twitter[w]\n",
    "    except KeyError:    \n",
    "        return result\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reweitghting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be competed and tested after assignment 4. Interesting to see if the intution will be true.\n",
    "\n",
    "def ttest(df):\n",
    "    all_sum_df = df.sum().sum()\n",
    "    p_df_ij = np.outer((df.sum(axis=1) / all_sum_df), \n",
    "                       (df.sum(axis=0) / all_sum_df))\n",
    "    \n",
    "    return ((df / all_sum_df) - p_df_ij) / np.sqrt(p_df_ij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining words into inputs (feature representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_concatenate(u, v):\n",
    "    return np.concatenate((u, v))\n",
    "\n",
    "def hypothesis_only(u, v):\n",
    "    return np.array(v)\n",
    "\n",
    "def vec_diff(u, v):\n",
    "    return np.array(u - v)\n",
    "\n",
    "def vec_max(u, v):\n",
    "    return np.maximum(u, v)\n",
    "\n",
    "def vec_conc_max(u, v):\n",
    "    return np.concatenate((vec_concatenate(u, v), vec_max(u, v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 12, 15,  3,  5,  2, 10, 12, 15])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_conc_max([10, 12, 15], [3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TorchShallowNeuralClassifier(early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 133. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 1.0430300533771515"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.875     0.942     0.907      1732\n",
      "           1      0.502     0.302     0.378       334\n",
      "\n",
      "    accuracy                          0.839      2066\n",
      "   macro avg      0.689     0.622     0.642      2066\n",
      "weighted avg      0.815     0.839     0.822      2066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_experiment = nli.wordentail_experiment(\n",
    "    train_data=wordentail_data['train'],\n",
    "    assess_data=wordentail_data['dev'],\n",
    "    model=net,\n",
    "    vector_func=glove_vec50,\n",
    "    vector_combo_func=vec_concatenate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TorchShallowNeuralClassifier(early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 38. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 1.9960777908563614"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.870     0.917     0.893      1732\n",
      "           1      0.402     0.290     0.337       334\n",
      "\n",
      "    accuracy                          0.816      2066\n",
      "   macro avg      0.636     0.604     0.615      2066\n",
      "weighted avg      0.795     0.816     0.803      2066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# experiment with the baseline TorchShallowNeuralClassifier(early_stopping=True)\n",
    "# best result with GloVe 200 data set 0.644 (0: 0.896, 1: 0.392)\n",
    "experiment = nli.wordentail_experiment(\n",
    "    train_data=wordentail_data['train'],\n",
    "    assess_data=wordentail_data['dev'],\n",
    "    model=net,\n",
    "    vector_func=glove_vec200,\n",
    "    vector_combo_func=vec_concatenate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LogisticRegression(fit_intercept=True, solver='liblinear', multi_class='ovr') and param_grid\n",
    "\n",
    "**vec_concatenate(u, v)** - Best params: {'C': 0.8, 'penalty': 'l2'} - 0.626 (0: 0.896, 1: 0.357)\n",
    "\n",
    "**hypothesis_only(u, v)** - Best params: {'C': 0.6, 'penalty': 'l2'} - 0.629 (0: 0.898, 1: 0.359)\n",
    "\n",
    "**vec_diff(u, v)** - Best params: {'C': 0.6, 'penalty': 'l2'} - 0.569 (0: 0.906, 1: 0.232)\n",
    "\n",
    "**vec_max(u, v)** - Best params: {'C': 1.0, 'penalty': 'l2'} - 0.637 (0: 0.906, 1: 0.367)\n",
    "\n",
    "**vec_overlap** - Best params: {'C': 0.4, 'penalty': 'l1'} - 0.456 (0: 0.912, 1: 0.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_with_hyperparameter_search(X, y):\n",
    "\n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr')\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.4, 0.6, 0.8, 1.0],\n",
    "        'penalty': ['l1','l2']}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "            X, y, mod, param_grid=param_grid, cv=3)\n",
    "\n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 0.6, 'penalty': 'l2'}\n",
      "Best score: 0.652\n",
      "CPU times: user 24 s, sys: 493 ms, total: 24.5 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word__experiment_xval = nli_ext.wordentail_experiment(\n",
    "    train_data=wordentail_data['train'],\n",
    "    assess_data=wordentail_data['train'],\n",
    "    train_func=fit_softmax_with_hyperparameter_search,\n",
    "    vector_func=vec_combined,\n",
    "    vector_combo_func=vec_max,\n",
    "    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_experiment_xval_model = word__experiment_xval['model']\n",
    "del word__experiment_xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_optmised_word_experiment_xval_model(X, y):\n",
    "    word_experiment_xval_model.fit(X, y)\n",
    "    return word_experiment_xval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.894     0.906     0.900      1732\n",
      "           1      0.477     0.443     0.460       334\n",
      "\n",
      "    accuracy                          0.832      2066\n",
      "   macro avg      0.686     0.675     0.680      2066\n",
      "weighted avg      0.827     0.832     0.829      2066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = nli_ext.wordentail_experiment(\n",
    "    train_data=wordentail_data['train'],\n",
    "    assess_data=wordentail_data['dev'],\n",
    "    train_func=fit_optmised_word_experiment_xval_model,\n",
    "    vector_func=vec_combined,\n",
    "    vector_combo_func=vec_conc_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TorchDeepNeuralClassifier(TorchShallowNeuralClassifier):\n",
    "    def __init__(self, dropout_prob=0.7, **kwargs):\n",
    "        self.dropout_prob = dropout_prob\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build_graph(self):        \n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(self.input_dim),\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.Dropout(p=self.dropout_prob),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim, self.n_classes_)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TorchDeepNeuralClassifier(early_stopping=True, \n",
    "                                dropout_prob=0.03,\n",
    "                               hidden_dim=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 33. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 0.3293429762125015"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.891     0.930     0.910      1732\n",
      "           1      0.529     0.407     0.460       334\n",
      "\n",
      "    accuracy                          0.846      2066\n",
      "   macro avg      0.710     0.669     0.685      2066\n",
      "weighted avg      0.832     0.846     0.837      2066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vcm_experiment = nli.wordentail_experiment(\n",
    "        train_data=wordentail_data['train'],\n",
    "        assess_data=wordentail_data['dev'],\n",
    "        model=net,\n",
    "        vector_func=glove_vec200,\n",
    "        vector_combo_func=lambda u, v : \n",
    "            np.concatenate((vec_concatenate(u, v), vec_max(u, v))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_classifier():\n",
    "\n",
    "#    updated_wordentail_train = updated_wordentail_train_data\n",
    "    updated_wordentail_train = wordentail_data['train']\n",
    "    \n",
    "    vm_experiment = nli.wordentail_experiment(\n",
    "        train_data=updated_wordentail_train,\n",
    "        assess_data=wordentail_data['dev'],\n",
    "        model=net,\n",
    "        vector_func=glove_vec200,\n",
    "        vector_combo_func=vec_max,\n",
    "        verbose=False)\n",
    "\n",
    "    vd_experiment = nli.wordentail_experiment(\n",
    "        train_data=updated_wordentail_train,\n",
    "        assess_data=wordentail_data['dev'],\n",
    "        model=net,\n",
    "        vector_func=glove_vec200,\n",
    "        vector_combo_func=vec_diff,\n",
    "        verbose=False)        \n",
    "    \n",
    "    ho_experiment = nli.wordentail_experiment(\n",
    "        train_data=updated_wordentail_train,\n",
    "        assess_data=wordentail_data['dev'],\n",
    "        model=net,\n",
    "        vector_func=glove_vec200,\n",
    "        vector_combo_func=hypothesis_only,\n",
    "        verbose=False)        \n",
    "\n",
    "    vc_experiment = nli.wordentail_experiment(\n",
    "        train_data=updated_wordentail_train,\n",
    "        assess_data=wordentail_data['dev'],\n",
    "        model=net,\n",
    "        vector_func=glove_vec200,\n",
    "        vector_combo_func=vec_concatenate,\n",
    "        verbose=False)\n",
    "\n",
    "    vcm_experiment = nli.wordentail_experiment(\n",
    "        train_data=updated_wordentail_train,\n",
    "        assess_data=wordentail_data['dev'],\n",
    "        model=net,\n",
    "        vector_func=glove_vec200,\n",
    "        vector_combo_func=lambda u, v : \n",
    "            np.concatenate((vec_concatenate(u, v), vec_max(u, v))),\n",
    "        verbose=False)\n",
    "\n",
    "    vcmh_experiment = nli.wordentail_experiment(\n",
    "        train_data=updated_wordentail_train,\n",
    "        assess_data=wordentail_data['dev'],\n",
    "        model=net,\n",
    "        vector_func=glove_vec200,\n",
    "        vector_combo_func=lambda u, v : \n",
    "            np.concatenate((vec_concatenate(u, v), \n",
    "                            vec_max(u, v), \n",
    "                            hypothesis_only(u, v))),\n",
    "        verbose=False)    \n",
    "    \n",
    "    result = dict()\n",
    "    result[vec_concatenate.__name__] = vc_experiment['macro-F1']\n",
    "    result[hypothesis_only.__name__] = ho_experiment['macro-F1']\n",
    "    result[vec_max.__name__] = vm_experiment['macro-F1']\n",
    "    result[vec_diff.__name__] = vd_experiment['macro-F1']\n",
    "    result[\"concat + max\"] = vcm_experiment['macro-F1']\n",
    "    result[\"concat + max + hypo\"] = vcmh_experiment['macro-F1']\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 80. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 0.01555661961901933"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vec_concatenate': 0.63213666477322,\n",
       " 'hypothesis_only': 0.5630663397515601,\n",
       " 'vec_max': 0.6168334808839924,\n",
       " 'vec_diff': 0.6334149543986657,\n",
       " 'concat + max': 0.6774463453933357,\n",
       " 'concat + max + hypo': 0.6872631588129681}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_neural_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRNNSentenceEncoderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, prem_seqs, hyp_seqs, prem_lengths, hyp_lengths, y=None):\n",
    "        self.prem_seqs = prem_seqs\n",
    "        self.hyp_seqs = hyp_seqs\n",
    "        self.prem_lengths = prem_lengths\n",
    "        self.hyp_lengths = hyp_lengths\n",
    "        self.y = y\n",
    "        assert len(self.prem_seqs) == len(self.hyp_seqs)\n",
    "        assert len(self.hyp_seqs) == len(self.prem_lengths)\n",
    "        assert len(self.prem_lengths) == len(self.hyp_lengths)\n",
    "        if self.y is not None:\n",
    "            assert len(self.hyp_lengths) == len(self.y)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch = list(zip(*batch))\n",
    "        X_prem = torch.nn.utils.rnn.pad_sequence(batch[0], batch_first=True)\n",
    "        X_hyp = torch.nn.utils.rnn.pad_sequence(batch[1], batch_first=True)\n",
    "        prem_lengths = torch.tensor(batch[2])\n",
    "        hyp_lengths = torch.tensor(batch[3])\n",
    "        if len(batch) == 5:\n",
    "            y = torch.tensor(batch[4])\n",
    "            return X_prem, X_hyp, prem_lengths, hyp_lengths, y\n",
    "        else:\n",
    "            return X_prem, X_hyp, prem_lengths, hyp_lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prem_seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return (self.prem_seqs[idx], self.hyp_seqs[idx],\n",
    "                    self.prem_lengths[idx], self.hyp_lengths[idx])\n",
    "        else:\n",
    "            return (self.prem_seqs[idx], self.hyp_seqs[idx],\n",
    "                    self.prem_lengths[idx], self.hyp_lengths[idx],\n",
    "                    self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRNNSentenceEncoderClassifierModel(nn.Module):\n",
    "    def __init__(self, prem_rnn, hyp_rnn, output_dim):\n",
    "        super().__init__()\n",
    "        self.prem_rnn = prem_rnn\n",
    "        self.hyp_rnn = hyp_rnn\n",
    "        self.output_dim = output_dim\n",
    "        self.bidirectional = self.prem_rnn.bidirectional\n",
    "        # Doubled because we concatenate the final states of\n",
    "        # the premise and hypothesis RNNs:\n",
    "        self.classifier_dim = self.prem_rnn.hidden_dim * 2\n",
    "        # Bidirectionality doubles it again:\n",
    "        if self.bidirectional:\n",
    "            self.classifier_dim *= 2\n",
    "        self.classifier_layer = nn.Linear(\n",
    "            self.classifier_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, X_prem, X_hyp, prem_lengths, hyp_lengths):\n",
    "        # Premise:\n",
    "        _, prem_state = self.prem_rnn(X_prem, prem_lengths)\n",
    "        prem_state = self.get_batch_final_states(prem_state)\n",
    "        # Hypothesis:\n",
    "        _, hyp_state = self.hyp_rnn(X_hyp, hyp_lengths)\n",
    "        hyp_state = self.get_batch_final_states(hyp_state)\n",
    "        # Final combination:\n",
    "        state = torch.cat((prem_state, hyp_state), dim=1)\n",
    "        # Classifier layer:\n",
    "        logits = self.classifier_layer(state)\n",
    "        return logits\n",
    "\n",
    "    def get_batch_final_states(self, state):\n",
    "        if self.prem_rnn.rnn.__class__.__name__ == 'LSTM':\n",
    "            state = state[0].squeeze(0)\n",
    "        else:\n",
    "            state = state.squeeze(0)\n",
    "        if self.bidirectional:\n",
    "            state = torch.cat((state[0], state[1]), dim=1)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRNNSentenceEncoderClassifier(TorchRNNClassifier):\n",
    "\n",
    "    def build_dataset(self, X, y=None):\n",
    "        X_prem, X_hyp = zip(*X)\n",
    "        X_prem, prem_lengths = self._prepare_sequences(X_prem)\n",
    "        X_hyp, hyp_lengths = self._prepare_sequences(X_hyp)\n",
    "        if y is None:\n",
    "            return TorchRNNSentenceEncoderDataset(\n",
    "                X_prem, X_hyp, prem_lengths, hyp_lengths)\n",
    "        else:\n",
    "            self.classes_ = sorted(set(y))\n",
    "            self.n_classes_ = len(self.classes_)\n",
    "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
    "            y = [class2index[label] for label in y]\n",
    "            return TorchRNNSentenceEncoderDataset(\n",
    "                X_prem, X_hyp, prem_lengths, hyp_lengths, y)\n",
    "\n",
    "    def build_graph(self):\n",
    "        prem_rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "\n",
    "        hyp_rnn = TorchRNNModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=prem_rnn.embedding,  # Same embedding for both RNNs.\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            rnn_cell_class=self.rnn_cell_class,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            bidirectional=self.bidirectional,\n",
    "            freeze_embedding=self.freeze_embedding)\n",
    "\n",
    "        model = TorchRNNSentenceEncoderClassifierModel(\n",
    "            prem_rnn, hyp_rnn, output_dim=self.n_classes_)\n",
    "\n",
    "        self.embed_dim = prem_rnn.embed_dim\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    vocab = wordentail_data['vocab']\n",
    "    vocab.append(\"$UNK\")\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TorchRNNSentenceEncoderClassifier(\n",
    "        get_vocab(),\n",
    "        max_iter=1000,\n",
    "        embed_dim=100,\n",
    "        bidirectional=True,\n",
    "        hidden_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 128. Training loss did not improve more than tol=1e-05. Final error is 3.32230406999588."
     ]
    }
   ],
   "source": [
    "X, y = zip(*updated_wordentail_train_data)\n",
    "net.fit(X, y)\n",
    "\n",
    "X_test, y_test = zip(*wordentail_data['dev'])\n",
    "predictions = net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.840     0.788     0.813      1732\n",
      "           1      0.168     0.222     0.191       334\n",
      "\n",
      "    accuracy                          0.697      2066\n",
      "   macro avg      0.504     0.505     0.502      2066\n",
      "weighted avg      0.731     0.697     0.713      2066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run200():\n",
    "    net = TorchRNNSentenceEncoderClassifier(\n",
    "            get_vocab(),\n",
    "            max_iter=1000,\n",
    "            embed_dim=200,\n",
    "            bidirectional=True,\n",
    "            hidden_dim=200)\n",
    "\n",
    "    X, y = zip(*wordentail_data['train'])\n",
    "    net.fit(X, y)\n",
    "\n",
    "    X_test, y_test = zip(*wordentail_data['dev'])\n",
    "    predictions = net.predict(X_test)\n",
    "    return classification_report(y_test, predictions, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 105. Training loss did not improve more than tol=1e-05. Final error is 0.8682949841022491."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.842     0.845     0.844      1732\n",
      "           1      0.183     0.180     0.181       334\n",
      "\n",
      "    accuracy                          0.738      2066\n",
      "   macro avg      0.513     0.512     0.513      2066\n",
      "weighted avg      0.736     0.738     0.737      2066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report200 = run200()\n",
    "print(report200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run300():\n",
    "    net = TorchRNNSentenceEncoderClassifier(\n",
    "            get_vocab(),\n",
    "            max_iter=1000,\n",
    "            embed_dim=200,\n",
    "            bidirectional=True,\n",
    "            hidden_dim=200)\n",
    "\n",
    "    X, y = zip(*wordentail_data['train'])\n",
    "    net.fit(X, y)\n",
    "\n",
    "    X_test, y_test = zip(*wordentail_data['dev'])\n",
    "    predictions = net.predict(X_test)\n",
    "    return classification_report(y_test, predictions, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run300' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-09359ec8df52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreport300\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun300\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run300' is not defined"
     ]
    }
   ],
   "source": [
    "report300 = run300()\n",
    "print(report300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_model = MLPClassifier(\n",
    "    alpha=0.7, learning_rate_init=0.01, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mlp_with_hyperparameter_search(X, y):\n",
    "\n",
    "    mod = MLPClassifier(\n",
    "        alpha=0.7, learning_rate_init=0.01, max_iter=100)\n",
    "\n",
    "    param_grid = {\n",
    "        'alpha': [0.3, 0.5, 0.7, 0.8, 0.9, 1.0],\n",
    "        'learning_rate_init': [0.0001, 0.001, 0.01, 0.1 ]}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "            X, y, mod, param_grid=param_grid, cv=3)\n",
    "\n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'alpha': 0.3, 'learning_rate_init': 0.001}\n",
      "Best score: 0.922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.914     0.827     0.868      1732\n",
      "           1      0.399     0.596     0.478       334\n",
      "\n",
      "    accuracy                          0.789      2066\n",
      "   macro avg      0.656     0.711     0.673      2066\n",
      "weighted avg      0.831     0.789     0.805      2066\n",
      "\n",
      "CPU times: user 10min 9s, sys: 5.7 s, total: 10min 15s\n",
      "Wall time: 10min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlpc_experiment = nli_ext.wordentail_experiment(\n",
    "    train_data=updated_wordentail_train_data,\n",
    "    assess_data=wordentail_data['dev'],\n",
    "    train_func=fit_mlp_with_hyperparameter_search,\n",
    "    vector_func=glove_vec200,\n",
    "    vector_combo_func=lambda u, v : \n",
    "        np.concatenate((vec_concatenate(u, v), vec_max(u, v)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_trained_model = mlpc_experiment['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlpc_trained_model(X, y):\n",
    "    mlpc_trained_model.max_iter = 100\n",
    "    mlpc_trained_model.fit(X, y)\n",
    "    return mlpc_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.908     0.816     0.860      1732\n",
      "           1      0.375     0.572     0.453       334\n",
      "\n",
      "    accuracy                          0.777      2066\n",
      "   macro avg      0.642     0.694     0.656      2066\n",
      "weighted avg      0.822     0.777     0.794      2066\n",
      "\n",
      "CPU times: user 19.4 s, sys: 253 ms, total: 19.6 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlpc_experiment = nli_ext.wordentail_experiment(\n",
    "    train_data=updated_wordentail_train_data,\n",
    "    assess_data=wordentail_data['dev'],\n",
    "    train_func=get_mlpc_trained_model,\n",
    "    vector_func=glove_vec200,\n",
    "    vector_combo_func=lambda u, v : \n",
    "        np.concatenate((vec_concatenate(u, v), vec_max(u, v)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.886     0.943     0.914      1732\n",
      "           1      0.559     0.371     0.446       334\n",
      "\n",
      "    accuracy                          0.851      2066\n",
      "   macro avg      0.722     0.657     0.680      2066\n",
      "weighted avg      0.833     0.851     0.838      2066\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antongochev/opt/miniconda3/envs/nlu/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlpc_experiment = nli.wordentail_experiment(\n",
    "    train_data=wordentail_data['train'],\n",
    "    assess_data=wordentail_data['dev'],\n",
    "    model=mlpc_trained_model,\n",
    "    vector_func=glove_vec200,\n",
    "    vector_combo_func=lambda u, v : \n",
    "        np.concatenate((vec_concatenate(u, v), vec_max(u, v)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_model = MLPClassifier(\n",
    "    alpha=0.2, learning_rate_init=0.001, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.880     0.956     0.916      1732\n",
      "           1      0.584     0.323     0.416       334\n",
      "\n",
      "    accuracy                          0.853      2066\n",
      "   macro avg      0.732     0.639     0.666      2066\n",
      "weighted avg      0.832     0.853     0.835      2066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlpc_experiment = nli.wordentail_experiment(\n",
    "    train_data=wordentail_data['train'],\n",
    "    assess_data=wordentail_data['dev'],\n",
    "    model=mlpc_model,\n",
    "    vector_func=vec_combined,\n",
    "    vector_combo_func= lambda u, v : \n",
    "        np.concatenate((vec_concatenate(u, v), vec_max(u, v)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_combined(w):\n",
    "    result = glove200.get(w)\n",
    "    if result is not None:\n",
    "        return result\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        result = glove_twitter[w]\n",
    "    except KeyError:    \n",
    "        return result\n",
    "\n",
    "    return result\n",
    "\n",
    "non_in_list = list()\n",
    "for (w1, w2), index in wordentail_data['train']:\n",
    "    word = vec_combined(w1)\n",
    "    if word is None:\n",
    "        non_in_list.append(w1)\n",
    "        \n",
    "    word = vec_combined(w2)\n",
    "    if word is None:\n",
    "        non_in_list.append(w2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_in_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TorchRNNClassifier \n",
    "\n",
    "Chained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_simple_chained_rnn_hp_search(X, y):\n",
    "    \n",
    "    model = TorchRNNClassifier(\n",
    "        get_vocab(),\n",
    "        hidden_dim=200,\n",
    "        embed_dim=200,\n",
    "        bidirectional=True,\n",
    "        early_stopping=True,\n",
    "        max_iter=1\n",
    "    )\n",
    "    \n",
    "    pg = {\n",
    "        'batch_size': [32, 64],\n",
    "        'eta': [0.001, 0.01]\n",
    "    }\n",
    "    \n",
    "    bestmod = utils.fit_classifier_with_hyperparameter_search(\n",
    "        X, y, model, param_grid=pg, cv=3)\n",
    "    \n",
    "    return bestmod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# chained_rnn_xval = nli_ext.wordentail_experiment(\n",
    "#     train_data=wordentail_data['train'],\n",
    "#     assess_data=wordentail_data['train'],\n",
    "#     train_func=fit_simple_chained_rnn_hp_search,\n",
    "#     vector_func=glove_vec200,\n",
    "#     vector_combo_func=vec_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "chained_rnn_xval_model = chained_rnn_xval['model']\n",
    "del chained_rnn_xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_optmised_chained_rnn_xval_model(X, y):\n",
    "    chained_rnn_xval_model.max_iter = 10\n",
    "    chained_rnn_xval_model.fit(X, y)\n",
    "    return chained_rnn_xval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 10 of 10; error is 100.30287890136242"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.838     1.000     0.912      1732\n",
      "           1      0.000     0.000     0.000       334\n",
      "\n",
      "    accuracy                          0.838      2066\n",
      "   macro avg      0.419     0.500     0.456      2066\n",
      "weighted avg      0.703     0.838     0.765      2066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result = nli_ext.wordentail_experiment(\n",
    "#     train_data=wordentail_data['train'],\n",
    "#     assess_data=wordentail_data['dev'],\n",
    "#     train_func=fit_optmised_chained_rnn_xval_model,\n",
    "#     vector_func=glove_vec50,\n",
    "#     vector_combo_func=vec_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TorchRNNClassifier(\n",
    "        get_vocab(),\n",
    "        hidden_dim=100,\n",
    "        embed_dim=50,\n",
    "        bidirectional=True,\n",
    "        early_stopping=True,\n",
    "        eta=0.01\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping after epoch 12. Validation score did not improve by tol=1e-05 for more than 10 epochs. Final error is 3.4910351037979126"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.838     1.000     0.912      1732\n",
      "           1      0.000     0.000     0.000       334\n",
      "\n",
      "    accuracy                          0.838      2066\n",
      "   macro avg      0.419     0.500     0.456      2066\n",
      "weighted avg      0.703     0.838     0.765      2066\n",
      "\n",
      "CPU times: user 16min 18s, sys: 2min 48s, total: 19min 7s\n",
      "Wall time: 6min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antongochev/opt/miniconda3/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# experiment = nli.wordentail_experiment(\n",
    "#     train_data=wordentail_data['train'],\n",
    "#     assess_data=wordentail_data['dev'],\n",
    "#     model=net,\n",
    "#     vector_func=glove_vec50,\n",
    "#     vector_combo_func=vec_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
